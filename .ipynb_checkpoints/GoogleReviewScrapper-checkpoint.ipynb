{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ab835ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import traceback\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "\n",
    "GM_WEBPAGE = 'https://www.google.com/maps/'\n",
    "MAX_WAIT = 20\n",
    "MAX_RETRY = 5\n",
    "MAX_SCROLLS = 40\n",
    "\n",
    "class GoogleMapsScraper:\n",
    "\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "        self.driver = self.__get_driver()\n",
    "        self.logger = self.__get_logger()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, tb):\n",
    "        if exc_type is not None:\n",
    "            traceback.print_exception(exc_type, exc_value, tb)\n",
    "\n",
    "        self.driver.close()\n",
    "        self.driver.quit()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def sort_by(self, url, ind):\n",
    "\n",
    "        self.driver.get(url)\n",
    "        self.__click_on_cookie_agreement()\n",
    "\n",
    "        wait = WebDriverWait(self.driver, MAX_WAIT)\n",
    "\n",
    "        # open dropdown menu\n",
    "        clicked = False\n",
    "        tries = 0\n",
    "        while not clicked and tries < MAX_RETRY:\n",
    "            try:\n",
    "                menu_bt = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@data-value=\\'Sort\\']')))\n",
    "                menu_bt.click()\n",
    "\n",
    "                clicked = True\n",
    "                time.sleep(3)\n",
    "            except Exception as e:\n",
    "                tries += 1\n",
    "                self.logger.warning('Failed to click sorting button')\n",
    "\n",
    "            # failed to open the dropdown\n",
    "            if tries == MAX_RETRY:\n",
    "                return -1\n",
    "\n",
    "        #  element of the list specified according to ind\n",
    "        recent_rating_bt = self.driver.find_elements_by_xpath('//div[@role=\\'menuitemradio\\']')[ind]\n",
    "        recent_rating_bt.click()\n",
    "\n",
    "        # wait to load review (ajax call)\n",
    "        time.sleep(5)\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def get_places(self, method='urls', keyword_list=None):\n",
    "\n",
    "        df_places = pd.DataFrame()\n",
    "\n",
    "        if method == 'urls':\n",
    "            # search_point_url = row['url']  # TODO:\n",
    "            pass\n",
    "        if method == 'squares':\n",
    "            search_point_url_list = self._gen_search_points_from_square(keyword_list=keyword_list)\n",
    "        else:\n",
    "            # search_point_url = f\"https://www.google.com/maps/search/{row['keyword']}/@{str(row['longitude'])},{str(row['latitude'])},{str(row['zoom'])}z\"\n",
    "            # TODO:\n",
    "            pass\n",
    "\n",
    "        for i, search_point_url in enumerate(search_point_url_list):\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f\"{i}/{len(search_point_url_list)}\")\n",
    "                df_places = df_places[['search_point_url', 'href', 'name', 'rating', 'num_reviews', 'close_time', 'other']]\n",
    "                df_places.to_csv('output/places_wax.csv', index=False)\n",
    "\n",
    "\n",
    "            try:\n",
    "                self.driver.get(search_point_url)\n",
    "            except NoSuchElementException:\n",
    "                self.driver.quit()\n",
    "                self.driver = self.__get_driver()\n",
    "                self.driver.get(search_point_url)\n",
    "\n",
    "            # Gambiarra to load all places into the page\n",
    "            scrollable_div = self.driver.find_element_by_css_selector(\n",
    "                \"div.siAUzd-neVct.section-scrollbox.cYB2Ge-oHo7ed.cYB2Ge-ti6hGc > div[aria-label*='Results for']\")\n",
    "            for i in range(10):\n",
    "                self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "\n",
    "            # Get places names and href\n",
    "            # time.sleep(2)\n",
    "            response = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            div_places = response.select('div[jsaction] > a[href]')\n",
    "            for div_place in div_places:\n",
    "                place_info = {\n",
    "                    'search_point_url': search_point_url.replace('https://www.google.com/maps/search/', ''),\n",
    "                    'href': div_place['href'],\n",
    "                    'name': div_place['aria-label'],\n",
    "                    'rating': None,\n",
    "                    'num_reviews': None,\n",
    "                    'close_time': None,\n",
    "                    'other': None\n",
    "                }\n",
    "\n",
    "                df_places = df_places.append(place_info, ignore_index=True)\n",
    "        df_places = df_places[['search_point_url', 'href', 'name', 'rating', 'num_reviews', 'close_time', 'other']]\n",
    "        df_places.to_csv('output/places_wax.csv', index=False)\n",
    "        self.driver.quit()\n",
    "\n",
    "    def _gen_search_points_from_square(self, keyword_list=None):\n",
    "        # TODO: Generate search points from corners of square\n",
    "\n",
    "        keyword_list = [] if keyword_list is None else keyword_list\n",
    "\n",
    "        square_points = pd.read_csv('input/square_points.csv')\n",
    "\n",
    "        cities = square_points['city'].unique()\n",
    "\n",
    "        search_urls = []\n",
    "\n",
    "        for city in cities:\n",
    "\n",
    "            df_aux = square_points[square_points['city'] == city]\n",
    "            latitudes = np.linspace(df_aux['latitude'].min(), df_aux['latitude'].max(), num=20)\n",
    "            longitudes = np.linspace(df_aux['longitude'].min(), df_aux['longitude'].max(), num=20)\n",
    "            coordinates_list = list(itertools.product(latitudes, longitudes, keyword_list))\n",
    "\n",
    "            search_urls += [f\"https://www.google.com/maps/search/{coordinates[2]}/@{str(coordinates[1])},{str(coordinates[0])},{str(15)}z\"\n",
    "             for coordinates in coordinates_list]\n",
    "\n",
    "        return search_urls\n",
    "\n",
    "\n",
    "\n",
    "    def get_reviews(self, offset):\n",
    "        # scroll to load reviews\n",
    "\n",
    "        # wait for other reviews to load (ajax)\n",
    "        time.sleep(4)\n",
    "\n",
    "        self.__scroll()\n",
    "\n",
    "\n",
    "        # expand review text\n",
    "        self.__expand_reviews()\n",
    "\n",
    "        # parse reviews\n",
    "        response = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        # TODO: Subject to changes\n",
    "        rblock = response.find_all('div', class_='jftiEf fontBodyMedium')\n",
    "        parsed_reviews = []\n",
    "        for index, review in enumerate(rblock):\n",
    "            if index >= offset:\n",
    "                parsed_reviews.append(self.__parse(review))\n",
    "        return parsed_reviews\n",
    "\n",
    "\n",
    "    def get_account(self, url):\n",
    "        self.driver.get(url)\n",
    "\n",
    "        # ajax call also for this section\n",
    "        time.sleep(4)\n",
    "\n",
    "        resp = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "\n",
    "        place_data = self.__parse_place(resp)\n",
    "        return place_data\n",
    "\n",
    "\n",
    "    def __parse(self, review):\n",
    "        item = {}\n",
    "\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            id_review = review['data-review-id']\n",
    "        except Exception as e:\n",
    "            id_review = None\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            username = review['aria-label']\n",
    "        except Exception as e:\n",
    "            username = None\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            review_text = self.__filter_string(review.find('span', class_='wiI7pd').text)\n",
    "        except Exception as e:\n",
    "            review_text = None\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            rating = float(review.find('span', class_='kvMYJc')['aria-label'].split(' ')[1])\n",
    "        except Exception as e:\n",
    "            rating = None\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            relative_date = review.find('span', class_='rsqaWe').text\n",
    "        except Exception as e:\n",
    "            relative_date = None\n",
    "\n",
    "        try:\n",
    "            n_reviews_photos = review.find('div', class_='section-review-subtitle').find_all('span')[1].text\n",
    "            metadata = n_reviews_photos.split('\\xe3\\x83\\xbb')\n",
    "            if len(metadata) == 3:\n",
    "                n_photos = int(metadata[2].split(' ')[0].replace('.', ''))\n",
    "            else:\n",
    "                n_photos = 0\n",
    "\n",
    "            idx = len(metadata)\n",
    "            n_reviews = int(metadata[idx - 1].split(' ')[0].replace('.', ''))\n",
    "\n",
    "        except Exception as e:\n",
    "            n_reviews = 0\n",
    "            n_photos = 0\n",
    "\n",
    "        try:\n",
    "            user_url = review.find('a')['href']\n",
    "        except Exception as e:\n",
    "            user_url = None\n",
    "\n",
    "        item['id_review'] = id_review\n",
    "        item['caption'] = review_text\n",
    "\n",
    "        # depends on language, which depends on geolocation defined by Google Maps\n",
    "        # custom mapping to transform into date should be implemented\n",
    "        item['relative_date'] = relative_date\n",
    "\n",
    "        # store datetime of scraping and apply further processing to calculate\n",
    "        # correct date as retrieval_date - time(relative_date)\n",
    "        item['retrieval_date'] = datetime.now()\n",
    "        item['rating'] = rating\n",
    "        item['username'] = username\n",
    "        item['n_review_user'] = n_reviews\n",
    "        item['n_photo_user'] = n_photos\n",
    "        item['url_user'] = user_url\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __parse_place(self, response):\n",
    "        place = {}\n",
    "        try:\n",
    "            place['overall_rating'] = float(response.find('div', class_='gm2-display-2').text.replace(',', '.'))\n",
    "        except:\n",
    "            place['overall_rating'] = 'NOT FOUND'\n",
    "\n",
    "        try:\n",
    "            place['n_reviews'] = int(response.find('div', class_='gm2-caption').text.replace('.', '').replace(',','').split(' ')[0])\n",
    "        except:\n",
    "            place['n_reviews'] = 0\n",
    "        \n",
    "        return place\n",
    "\n",
    "    # expand review description\n",
    "    def __expand_reviews(self):\n",
    "        # use XPath to load complete reviews\n",
    "        # TODO: Subject to changes\n",
    "        links = self.driver.find_elements_by_xpath('//button[@jsaction=\"pane.review.expandReview\"]')\n",
    "        for l in links:\n",
    "            l.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "\n",
    "    def __scroll(self):\n",
    "        # TODO: Subject to changes\n",
    "        scrollable_div = self.driver.find_element_by_css_selector('div.m6QErb.DxyBCb.kA9KIf.dS8AEf')\n",
    "        self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "        #self.driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "\n",
    "\n",
    "    def __get_logger(self):\n",
    "        # create logger\n",
    "        logger = logging.getLogger('googlemaps-scraper')\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "\n",
    "        # create console handler and set level to debug\n",
    "        fh = logging.FileHandler('gm-scraper.log')\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "\n",
    "        # create formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "        # add formatter to ch\n",
    "        fh.setFormatter(formatter)\n",
    "\n",
    "        # add ch to logger\n",
    "        logger.addHandler(fh)\n",
    "        return logger\n",
    "\n",
    "\n",
    "    def __get_driver(self, debug=False):\n",
    "        options = Options()\n",
    "\n",
    "        if not self.debug:\n",
    "            options.add_argument(\"--headless\")\n",
    "        else:\n",
    "            options.add_argument(\"--window-size=1366,768\")\n",
    "\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--lang=en-GB\")\n",
    "        input_driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=options)\n",
    "\n",
    "         # click on google agree button so we can continue (not needed anymore)\n",
    "         # EC.element_to_be_clickable((By.XPATH, '//span[contains(text(), \"I agree\")]')))\n",
    "        input_driver.get(GM_WEBPAGE)\n",
    "        return input_driver\n",
    "\n",
    "    # cookies agreement click\n",
    "    def __click_on_cookie_agreement(self):\n",
    "        try:\n",
    "            agree = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//span[contains(text(), \"Reject all\")]')))\n",
    "            agree.click()\n",
    "\n",
    "            # back to the main page\n",
    "            # self.driver.switch_to_default_content()\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    # util function to clean special characters\n",
    "    def __filter_string(self, str):\n",
    "        strOut = str.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "        return strOut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "69023a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "collection = [];\n",
    "class ReviewGatherer:\n",
    "    def __init__(self, url_file, from_date):\n",
    "\n",
    "        # load urls file\n",
    "        with open(url_file, 'r') as furl:\n",
    "            self.urls = [u[:-1] for u in furl]\n",
    "\n",
    "        # min date review to scrape\n",
    "        self.min_date_review = datetime.strptime(from_date, '%Y-%m-%d')\n",
    "\n",
    "        # logging\n",
    "        self.logger = self.__get_logger()\n",
    "\n",
    "    def scrape_gm_reviews(self):\n",
    "        # init scraper and incremental add reviews\n",
    "        # TO DO: pass logger as parameter to log into one single file?\n",
    "        with GoogleMapsScraper(debug=True) as scraper:\n",
    "            for url in self.urls:\n",
    "                try:\n",
    "                    #ind = {'most_relevant' : 0 , 'newest' : 1, 'highest_rating' : 2, 'lowest_rating' : 3 }\n",
    "                    error = scraper.sort_by(url, 1)\n",
    "                    \n",
    "                    if error == 0:\n",
    "                        stop = False\n",
    "                        offset = 0\n",
    "                        n_new_reviews = 0\n",
    "                        while not stop:\n",
    "                            rlist = scraper.get_reviews(offset)\n",
    "                            for r in rlist:\n",
    "                                # calculate review date and compare to input min_date_review\n",
    "                                r['timestamp'] = self.__parse_relative_date(r['relative_date'])\n",
    "                                stop = self.__stop(r)\n",
    "                                if not stop:\n",
    "                                    collection.append(r)\n",
    "                                    n_new_reviews += 1\n",
    "                                else:\n",
    "                                    break\n",
    "                            offset += len(rlist)\n",
    "\n",
    "                        # log total number\n",
    "                        self.logger.info('{} : {} new reviews'.format(url, n_new_reviews))\n",
    "                    else:\n",
    "                        self.logger.warning('Sorting reviews failed for {}'.format(url))\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(\"Exception\")\n",
    "                    print(e)\n",
    "        return(collection)\n",
    "\n",
    "\n",
    "    def __parse_relative_date(self, string_date):\n",
    "        curr_date = datetime.now()\n",
    "        split_date = string_date.split(' ')\n",
    "\n",
    "        n = split_date[0]\n",
    "        delta = split_date[1]\n",
    "\n",
    "        if delta == 'year':\n",
    "            return curr_date - timedelta(days=365)\n",
    "        elif delta == 'years':\n",
    "            return curr_date - timedelta(days=365 * int(n))\n",
    "        elif delta == 'month':\n",
    "            return curr_date - timedelta(days=30)\n",
    "        elif delta == 'months':\n",
    "            return curr_date - timedelta(days=30 * int(n))\n",
    "        elif delta == 'week':\n",
    "            return curr_date - timedelta(weeks=1)\n",
    "        elif delta == 'weeks':\n",
    "            return curr_date - timedelta(weeks=int(n))\n",
    "        elif delta == 'day':\n",
    "            return curr_date - timedelta(days=1)\n",
    "        elif delta == 'days':\n",
    "            return curr_date - timedelta(days=int(n))\n",
    "        elif delta == 'hour':\n",
    "            return curr_date - timedelta(hours=1)\n",
    "        elif delta == 'hours':\n",
    "            return curr_date - timedelta(hours=int(n))\n",
    "        elif delta == 'minute':\n",
    "            return curr_date - timedelta(minutes=1)\n",
    "        elif delta == 'minutes':\n",
    "            return curr_date - timedelta(minutes=int(n))\n",
    "        elif delta == 'moments':\n",
    "            return curr_date - timedelta(seconds=1)\n",
    "\n",
    "\n",
    "    def __stop(self, r):\n",
    "        review_list = [x for x in collection if x['id_review'] == r['id_review']];\n",
    "        is_old_review = review_list[0] if len(review_list) != 0 else None\n",
    "        \n",
    "        if is_old_review is None and r['timestamp'] >= self.min_date_review:\n",
    "            print('review is new')\n",
    "            return False\n",
    "        else:\n",
    "            print('review is old')\n",
    "            return True\n",
    "\n",
    "    def __get_logger(self):\n",
    "        # create logger\n",
    "        logger = logging.getLogger('monitor')\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        # create console handler and set level to debug\n",
    "        fh = logging.FileHandler('monitor.log')\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        # create formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        # add formatter to ch\n",
    "        fh.setFormatter(formatter)\n",
    "        # add ch to logger\n",
    "        logger.addHandler(fh)\n",
    "\n",
    "        return logger\n",
    "\n",
    "    #parser = argparse.ArgumentParser(description='Monitor Google Maps places')\n",
    "    #parser.add_argument('--i', type=str, default='urls.txt', help='target URLs file')\n",
    "    #parser.add_argument('--from-date', type=str) # start date in format: YYYY-MM-DD\n",
    "\n",
    "    #args = parser.parse_args()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "65ffbbc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "review is new\n",
      "Exception\n",
      "'dict' object has no attribute 'id_review'\n",
      "[{'id_review': 'ChZDSUhNMG9nS0VJQ0FnSURCeWIza1FBEAE', 'caption': '', 'relative_date': 'a day ago', 'retrieval_date': datetime.datetime(2023, 1, 24, 0, 37, 59, 709738), 'rating': 5.0, 'username': 'H S', 'n_review_user': 0, 'n_photo_user': 0, 'url_user': 'https://www.google.com/maps/contrib/115518838229771297793/reviews?hl=en-US', 'timestamp': datetime.datetime(2023, 1, 23, 0, 37, 59, 717682)}]\n"
     ]
    }
   ],
   "source": [
    "monitor = ReviewGatherer(\"urls.text\", '2021-01-01')\n",
    "\n",
    "try:\n",
    "    items = monitor.scrape_gm_reviews()\n",
    "    print(items);\n",
    "except Exception as e:\n",
    "    monitor.logger.error('Not handled error: {}'.format(e))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c15b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1536de25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
