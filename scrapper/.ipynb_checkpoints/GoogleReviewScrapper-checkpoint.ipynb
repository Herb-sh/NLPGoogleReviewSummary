{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab835ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import pandas as pd\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime\n",
    "import time\n",
    "import re\n",
    "import logging\n",
    "import traceback\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "GM_WEBPAGE = 'https://www.google.com/maps/'\n",
    "MAX_WAIT = 25\n",
    "MAX_RETRY = 5\n",
    "MAX_SCROLLS = 40\n",
    "\n",
    "class GoogleMapsScraper:\n",
    "\n",
    "    def __init__(self, debug=False):\n",
    "        self.debug = debug\n",
    "        self.driver = self.__getDriver()\n",
    "\n",
    "    def __enter__(self):\n",
    "        return self\n",
    "\n",
    "    def __exit__(self, exc_type, exc_value, tb):\n",
    "        if exc_type is not None:\n",
    "            traceback.print_exception(exc_type, exc_value, tb)\n",
    "\n",
    "        self.driver.close()\n",
    "        self.driver.quit()\n",
    "\n",
    "        return True\n",
    "\n",
    "    def sortBy(self, url, ind):\n",
    "        self.driver.get(url)\n",
    "        \n",
    "        self.__clickOnCookieAgreement()\n",
    "\n",
    "        wait = WebDriverWait(self.driver, MAX_WAIT)\n",
    "\n",
    "        # open dropdown menu\n",
    "        clicked = False\n",
    "        tries = 0\n",
    "        while not clicked and tries < MAX_RETRY:\n",
    "            try:\n",
    "                menu_bt = wait.until(EC.element_to_be_clickable((By.XPATH, '//button[@data-value=\\'Sort\\']')))\n",
    "                menu_bt.click()\n",
    "\n",
    "                clicked = True\n",
    "                time.sleep(3)\n",
    "            except Exception as e:\n",
    "                tries += 1\n",
    "\n",
    "            # failed to open the dropdown\n",
    "            if tries == MAX_RETRY:\n",
    "                return -1\n",
    "\n",
    "        #  element of the list specified according to ind\n",
    "        recent_rating_bt = self.driver.find_elements_by_xpath('//div[@role=\\'menuitemradio\\']')[ind]\n",
    "        recent_rating_bt.click()\n",
    "\n",
    "        # wait to load review (ajax call)\n",
    "        time.sleep(5)\n",
    "\n",
    "        return 0\n",
    "\n",
    "    def getPlaces(self, method='urls', keyword_list=None):\n",
    "\n",
    "        df_places = pd.DataFrame()\n",
    "\n",
    "        if method == 'urls':\n",
    "            # search_point_url = row['url']  # TODO:\n",
    "            pass\n",
    "        if method == 'squares':\n",
    "            search_point_url_list = self._genSearchPointsFromSquare(keyword_list=keyword_list)\n",
    "        else:\n",
    "            # search_point_url = f\"https://www.google.com/maps/search/{row['keyword']}/@{str(row['longitude'])},{str(row['latitude'])},{str(row['zoom'])}z\"\n",
    "            # TODO:\n",
    "            pass\n",
    "\n",
    "        for i, search_point_url in enumerate(search_point_url_list):\n",
    "\n",
    "            if (i+1) % 10 == 0:\n",
    "                print(f\"{i}/{len(search_point_url_list)}\")\n",
    "                df_places = df_places[['search_point_url', 'href', 'name', 'rating', 'num_reviews', 'close_time', 'other']]\n",
    "                df_places.to_csv('output/places_wax.csv', index=False)\n",
    "\n",
    "\n",
    "            try:\n",
    "                self.driver.get(search_point_url)\n",
    "            except NoSuchElementException:\n",
    "                self.driver.quit()\n",
    "                self.driver = self.__getDriver()\n",
    "                self.driver.get(search_point_url)\n",
    "\n",
    "            # Gambiarra to load all places into the page\n",
    "            scrollable_div = self.driver.find_element_by_css_selector(\n",
    "                \"div.siAUzd-neVct.section-scrollbox.cYB2Ge-oHo7ed.cYB2Ge-ti6hGc > div[aria-label*='Results for']\")\n",
    "            for i in range(10):\n",
    "                self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "\n",
    "            # Get places names and href\n",
    "            # time.sleep(2)\n",
    "            response = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "            div_places = response.select('div[jsaction] > a[href]')\n",
    "            for div_place in div_places:\n",
    "                place_info = {\n",
    "                    'search_point_url': search_point_url.replace('https://www.google.com/maps/search/', ''),\n",
    "                    'href': div_place['href'],\n",
    "                    'name': div_place['aria-label'],\n",
    "                    'rating': None,\n",
    "                    'num_reviews': None,\n",
    "                    'close_time': None,\n",
    "                    'other': None\n",
    "                }\n",
    "\n",
    "                df_places = df_places.append(place_info, ignore_index=True)\n",
    "        df_places = df_places[['search_point_url', 'href', 'name', 'rating', 'num_reviews', 'close_time', 'other']]\n",
    "        df_places.to_csv('output/places_wax.csv', index=False)\n",
    "        self.driver.quit()\n",
    "\n",
    "    def _genSearchPointsFromSquare(self, keyword_list=None):\n",
    "        keyword_list = [] if keyword_list is None else keyword_list\n",
    "\n",
    "        square_points = pd.read_csv('input/square_points.csv')\n",
    "\n",
    "        cities = square_points['city'].unique()\n",
    "\n",
    "        search_urls = []\n",
    "\n",
    "        for city in cities:\n",
    "            df_aux = square_points[square_points['city'] == city]\n",
    "            latitudes = np.linspace(df_aux['latitude'].min(), df_aux['latitude'].max(), num=20)\n",
    "            longitudes = np.linspace(df_aux['longitude'].min(), df_aux['longitude'].max(), num=20)\n",
    "            coordinates_list = list(itertools.product(latitudes, longitudes, keyword_list))\n",
    "\n",
    "            search_urls += [f\"https://www.google.com/maps/search/{coordinates[2]}/@{str(coordinates[1])},{str(coordinates[0])},{str(15)}z\"\n",
    "             for coordinates in coordinates_list]\n",
    "\n",
    "        return search_urls\n",
    "\n",
    "\n",
    "\n",
    "    # Triggers scroll which then triggers loading of next patch of reviews\n",
    "    # It then expands reviews, query the DOM and iterates through each review to parse it\n",
    "    # Returns list of parsed reviews\n",
    "    def getReviews(self, offset):\n",
    "        # wait for other reviews to load (ajax)\n",
    "        time.sleep(4)\n",
    "\n",
    "        self.__scroll()\n",
    "\n",
    "        # expand review text\n",
    "        self.__expandReviews()\n",
    "\n",
    "        # parse reviews\n",
    "        response = BeautifulSoup(self.driver.page_source, 'html.parser')\n",
    "        \n",
    "        # TODO: Subject to changes\n",
    "        rblock = response.find_all('div', class_='jftiEf fontBodyMedium')\n",
    "        \n",
    "        parsed_reviews = []\n",
    "        for index, review in enumerate(rblock):\n",
    "            if index >= offset:\n",
    "                parsed_reviews.append(self.__parse(review))\n",
    "        return parsed_reviews\n",
    "\n",
    "\n",
    "    def __parse(self, review):\n",
    "        item = {}\n",
    "\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            id_review = review['data-review-id']\n",
    "        except Exception as e:\n",
    "            id_review = None\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            review_text = self.__filterString(review.find('span', class_='wiI7pd').text)\n",
    "        except Exception as e:\n",
    "            review_text = None\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            rating = float(review.find('span', class_='kvMYJc')['aria-label'].split(' ')[1])\n",
    "        except Exception as e:\n",
    "            rating = None\n",
    "\n",
    "        try:\n",
    "            # TODO: Subject to changes\n",
    "            relative_date = review.find('span', class_='rsqaWe').text\n",
    "        except Exception as e:\n",
    "            relative_date = None\n",
    "\n",
    "        item['id_review'] = id_review\n",
    "        item['caption'] = review_text\n",
    "        item['relative_date'] = relative_date\n",
    "        item['rating'] = rating\n",
    "\n",
    "        return item\n",
    "\n",
    "\n",
    "    def __parsePlace(self, response):\n",
    "        place = {}\n",
    "        try:\n",
    "            place['overall_rating'] = float(response.find('div', class_='gm2-display-2').text.replace(',', '.'))\n",
    "        except:\n",
    "            place['overall_rating'] = 'NOT FOUND'\n",
    "\n",
    "        try:\n",
    "            place['n_reviews'] = int(response.find('div', class_='gm2-caption').text.replace('.', '').replace(',','').split(' ')[0])\n",
    "        except:\n",
    "            place['n_reviews'] = 0\n",
    "        \n",
    "        return place\n",
    "\n",
    "    # expand review content - click on each expand button which then replaces the ellipsized text\n",
    "    # use XPath to load complete reviews\n",
    "    def __expandReviews(self):\n",
    "        # TODO: Subject to changes\n",
    "        links = self.driver.find_elements_by_xpath('//button[@jsaction=\"pane.review.expandReview\"]')\n",
    "        for l in links:\n",
    "            l.click()\n",
    "        wait = WebDriverWait(self.driver, 1) # 2\n",
    "            \n",
    "    # scroll the review container element to show the next patch of reviews\n",
    "    def __scroll(self):\n",
    "        scrollable_div = self.driver.find_element_by_css_selector('div.m6QErb.DxyBCb.kA9KIf.dS8AEf')\n",
    "        self.driver.execute_script('arguments[0].scrollTop = arguments[0].scrollHeight', scrollable_div)\n",
    "\n",
    "\n",
    "    def __getDriver(self, debug=False):\n",
    "        options = Options()\n",
    "\n",
    "        if not self.debug:\n",
    "            options.add_argument(\"--headless\")\n",
    "        else:\n",
    "            options.add_argument(\"--window-size=1366,768\")\n",
    "\n",
    "        options.add_argument(\"--disable-notifications\")\n",
    "        options.add_argument(\"--lang=en-GB\")\n",
    "        input_driver = webdriver.Chrome(executable_path=ChromeDriverManager().install(), options=options)\n",
    "\n",
    "         # click on google agree button so we can continue (not needed anymore)\n",
    "         # EC.element_to_be_clickable((By.XPATH, '//span[contains(text(), \"I agree\")]')))\n",
    "        input_driver.get(GM_WEBPAGE)\n",
    "        return input_driver\n",
    "\n",
    "    # cookies agreement click\n",
    "    def __clickOnCookieAgreement(self):\n",
    "        try:\n",
    "            agree = WebDriverWait(self.driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.XPATH, '//span[contains(text(), \"Reject all\")]')))\n",
    "            agree.click()\n",
    "\n",
    "            # back to the main page\n",
    "            # self.driver.switch_to_default_content()\n",
    "            return True\n",
    "        except:\n",
    "            return False\n",
    "\n",
    "    # util function to clean special characters\n",
    "    def __filterString(self, str):\n",
    "        strOut = str.replace('\\r', ' ').replace('\\n', ' ').replace('\\t', ' ')\n",
    "        return strOut\n",
    "    \n",
    "    def __getLogger(self):\n",
    "        # create logger\n",
    "        logger = logging.getLogger('googlemaps-scraper')\n",
    "        logger.setLevel(logging.DEBUG)\n",
    "        # create console handler and set level to debug\n",
    "        fh = logging.FileHandler('gm-scraper.log')\n",
    "        fh.setLevel(logging.DEBUG)\n",
    "        # create formatter\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        # add formatter to ch\n",
    "        fh.setFormatter(formatter)\n",
    "        # add ch to logger\n",
    "        logger.addHandler(fh)\n",
    "        return logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69023a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "from datetime import datetime, timedelta\n",
    "import argparse\n",
    "import logging\n",
    "import sys\n",
    "\n",
    "collection = [];\n",
    "class ReviewGatherer:\n",
    "    def __init__(self, url_file, from_date, max_items, debug):\n",
    "\n",
    "        # load urls file\n",
    "        with open(url_file, 'r') as furl:\n",
    "            self.urls = [u[:-1] for u in furl]\n",
    "\n",
    "        # min date review to scrape\n",
    "        self.min_date_review = datetime.strptime(from_date, '%Y-%m-%d')\n",
    "        \n",
    "        self.max_items = max_items;\n",
    "        \n",
    "        self.debug = debug\n",
    "\n",
    "    def scrapeGMReviews(self):\n",
    "        # init scraper and incremental add reviews\n",
    "        # TO DO: pass logger as parameter to log into one single file?\n",
    "        with GoogleMapsScraper(debug=self.debug) as scraper:\n",
    "            for url in self.urls:\n",
    "                try:\n",
    "                    #ind = {'most_relevant' : 0 , 'newest' : 1, 'highest_rating' : 2, 'lowest_rating' : 3 }\n",
    "                    error = scraper.sortBy(url, 1)\n",
    "                    \n",
    "                    if error == 0:\n",
    "                        stop = False\n",
    "                        offset = 0\n",
    "                        n_new_reviews = 0\n",
    "                        while not stop:\n",
    "                            rlist = scraper.getReviews(offset)\n",
    "                            for r in rlist:\n",
    "                                # calculate review date and compare to input min_date_review\n",
    "                                r['timestamp'] = self.__parseRelativeDate(r['relative_date'])\n",
    "                                stop = self.__stop(r)\n",
    "                                if not stop:\n",
    "                                    collection.append(r)\n",
    "                                    n_new_reviews += 1\n",
    "                                else:\n",
    "                                    break\n",
    "                            offset += len(rlist)\n",
    "                    else:\n",
    "                        print('Sorting reviews failed for {}'.format(url))\n",
    "                except Exception as e:\n",
    "                    print('Exception: {}'.format(e))\n",
    "                    \n",
    "        return(collection)\n",
    "\n",
    "\n",
    "    def __parseRelativeDate(self, string_date):\n",
    "        curr_date = datetime.now()\n",
    "        split_date = string_date.split(' ')\n",
    "\n",
    "        n = split_date[0]\n",
    "        delta = split_date[1]\n",
    "\n",
    "        if delta == 'year':\n",
    "            return curr_date - timedelta(days=365)\n",
    "        elif delta == 'years':\n",
    "            return curr_date - timedelta(days=365 * int(n))\n",
    "        elif delta == 'month':\n",
    "            return curr_date - timedelta(days=30)\n",
    "        elif delta == 'months':\n",
    "            return curr_date - timedelta(days=30 * int(n))\n",
    "        elif delta == 'week':\n",
    "            return curr_date - timedelta(weeks=1)\n",
    "        elif delta == 'weeks':\n",
    "            return curr_date - timedelta(weeks=int(n))\n",
    "        elif delta == 'day':\n",
    "            return curr_date - timedelta(days=1)\n",
    "        elif delta == 'days':\n",
    "            return curr_date - timedelta(days=int(n))\n",
    "        elif delta == 'hour':\n",
    "            return curr_date - timedelta(hours=1)\n",
    "        elif delta == 'hours':\n",
    "            return curr_date - timedelta(hours=int(n))\n",
    "        elif delta == 'minute':\n",
    "            return curr_date - timedelta(minutes=1)\n",
    "        elif delta == 'minutes':\n",
    "            return curr_date - timedelta(minutes=int(n))\n",
    "        elif delta == 'moments':\n",
    "            return curr_date - timedelta(seconds=1)\n",
    "\n",
    "\n",
    "    def __stop(self, r):\n",
    "        review_list = [x for x in collection if x['id_review'] == r['id_review']];\n",
    "        is_old_review = review_list[0] if len(review_list) != 0 else None\n",
    "        \n",
    "        if is_old_review is None and r['timestamp'] >= self.min_date_review and len(collection) < self.max_items:\n",
    "            return False\n",
    "        else:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65ffbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "gatherer = ReviewGatherer(\"../data/urls.text\", '2011-01-01', 500, True)\n",
    "\n",
    "try:\n",
    "    collection = gatherer.scrapeGMReviews()\n",
    "except Exception as e:\n",
    "    print('Not handled error: {}'.format(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c15b7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_collection = pd.DataFrame.from_records(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42ce7cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews = df_collection[['caption', 'rating', 'timestamp']]\n",
    "df_reviews.to_csv('../data/reviews.csv', sep=';', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6dde0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>caption</th>\n",
       "      <th>rating</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ganz solide Pizza. Leider wie alles overrated und overpriced!  (Translated by Google) Pretty solid pizza. Unfortunately like everything overrated and overpriced!</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2023-02-02 01:34:09.031362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What is not to like? Pizza? Good! Wine? Good? Nice people? Good!  Waiting time at the entrance is about 15-20 minutes. Orders are made at the COUNTER, don’t wait for the menu.</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-01-29 01:34:09.031402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(Translated by Google) This pizza restaurant was recommended and it was delicious. The pizza topped with pistachios is delicious :) …</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-01-29 01:34:09.031405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Mega leckere Pizza. Einer der besten Frankfurts.  (Translated by Google) …</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-01-29 01:34:09.031408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Phänomenale Pizza wie in Italien!  (Translated by Google) Phenomenal pizza like in Italy!</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2023-01-29 01:34:09.031410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                           caption  \\\n",
       "0                Ganz solide Pizza. Leider wie alles overrated und overpriced!  (Translated by Google) Pretty solid pizza. Unfortunately like everything overrated and overpriced!   \n",
       "2  What is not to like? Pizza? Good! Wine? Good? Nice people? Good!  Waiting time at the entrance is about 15-20 minutes. Orders are made at the COUNTER, don’t wait for the menu.   \n",
       "3                                            (Translated by Google) This pizza restaurant was recommended and it was delicious. The pizza topped with pistachios is delicious :) …   \n",
       "4                                                                                                       Mega leckere Pizza. Einer der besten Frankfurts.  (Translated by Google) …   \n",
       "5                                                                                        Phänomenale Pizza wie in Italien!  (Translated by Google) Phenomenal pizza like in Italy!   \n",
       "\n",
       "   rating                  timestamp  \n",
       "0     4.0 2023-02-02 01:34:09.031362  \n",
       "2     5.0 2023-01-29 01:34:09.031402  \n",
       "3     5.0 2023-01-29 01:34:09.031405  \n",
       "4     5.0 2023-01-29 01:34:09.031408  \n",
       "5     5.0 2023-01-29 01:34:09.031410  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_reviews.loc[df_reviews['caption'] != ''].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
